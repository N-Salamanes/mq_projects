{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Sentiment Analysis of Tweets (20 marks) - Update 18 April 2017\n",
    "\n",
    "In this assignment you will use statistical classifiers for the task of predicting the polarity (positive, negative) of opinions expressed in tweets. This is a type of **sentiment analysis** which is becoming increasingly useful given the strong influence of the opinions posted in social media nowadays.\n",
    "\n",
    "To learn more about sentiment analysis and some of its techniques you can read chapter 7 of this book:\n",
    "\n",
    "* [Maynard et al. Natural Language Processing for the Semantic Web. Morgan & Claypool, 2016.](http://www.morganclaypool.com/doi/10.2200/S00741ED1V01Y201611WBE015)\n",
    "\n",
    "For this assignment we will use the Twitter Sentiment Analysis Training Corpus described in [this blogpost](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/), which contains over 1.5 million tweets annotated with the sentiment polarity: 0 for negative sentiment, and 1 for positive sentiment.\n",
    "\n",
    "**Note that this corpus is much larger than the data that we have used in the workshop exercises. Be careful when you process it. If your code is not efficient, you will easily overwhelm the resources in your computer. To do the tasks, it is best that your computer has at least 16GB of RAM (the computers in E6A123 have 16GB of RAM). It would be wise if you did your first tests for debugging with a small portion of the data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the file SentimenAnalysisDataset.zip, unzip the file and place the resulting CSV (comma-separated-value) file in the same folder as this notebook. The unzipped file takes 150MB of space, so there is no problem to load the code into the memory of a modern computer. The following code uses Python's `csv` module to load the data and prints the first row and the total number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import csv\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header line: ['\\ufeffItemID', 'Sentiment', 'SentimentSource', 'SentimentText']\n",
      "['1', '0', 'Sentiment140', '                     is so sad for my APL friend.............']\n",
      "Total number of rows: 1578614\n"
     ]
    }
   ],
   "source": [
    "with open('Sentiment Analysis Dataset.csv', encoding='utf8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    print(\"Header line: %s\" % next(reader))\n",
    "    annotated_data = [r for r in reader]\n",
    "print(annotated_data[0])\n",
    "print(\"Total number of rows:\", len(annotated_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see, each element in the list `annotated_data` is a list with the following information:\n",
    "\n",
    "* item ID\n",
    "* Sentiment (0 if negative, 1 if positive)\n",
    "* Sentiment source (we can ignore this information for this assignment)\n",
    "* Text of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this exercise more manageable, we will use only 500,000 tweets taken randomly from the annotated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(annotated_data)\n",
    "annotated_data = annotated_data[:500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the tweets you will see that there are words in all uppercase characters, and that information may be useful. Therefore, **in the exercises below it is probably best that you do not convert the words to uppercase or lowercase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1 (1 mark) - Split the data\n",
    "Split the data into a training set, a dev-test set, and a test set. Use the following ratio for splitting the data:\n",
    "\n",
    "* Training set: 80%\n",
    "* Dev-test set: 10%\n",
    "* Test set: 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold1 = int(len(annotated_data)*0.8)\n",
    "threshold2 = int(len(annotated_data)*0.9)\n",
    "\n",
    "training_set = annotated_data[:threshold1]\n",
    "devtest_set = annotated_data[threshold1:threshold2]\n",
    "test_set = annotated_data[threshold2:]\n",
    "\n",
    "# tokenize the text in the sets\n",
    "training_set_tokenized = []\n",
    "devtest_set_tokenized = []\n",
    "test_set_tokenized = []\n",
    "\n",
    "for (i,j,k,l) in training_set:\n",
    "    training_set_tokenized.append((i,j,k,nltk.word_tokenize(l)))\n",
    "for (i,j,k,l) in devtest_set:\n",
    "    devtest_set_tokenized.append((i,j,k,nltk.word_tokenize(l)))\n",
    "for (i,j,k,l) in test_set:\n",
    "    test_set_tokenized.append((i,j,k,nltk.word_tokenize(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (1 mark) - Check that the data are balanced\n",
    "Print the percentage of positive and negative sentiments in each partition, and check that they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set positive percentage: 49.99%\n",
      "training set negative percentage: 50.01%\n",
      "devtest set positive percentage: 49.81%\n",
      "devtest set negative percentage: 50.19%\n",
      "test set positive percentage: 50.19%\n",
      "test set negative percentage: 49.81%\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "training_PosNeg = [j for (i,j,k,l) in training_set]\n",
    "training_PosNeg_Counter = Counter(training_PosNeg)\n",
    "# devtest\n",
    "devtest_PosNeg = [j for (i,j,k,l) in devtest_set]\n",
    "devtest_PosNeg_Counter = Counter(devtest_PosNeg)\n",
    "# testset\n",
    "test_PosNeg = [j for (i,j,k,l) in test_set]\n",
    "test_PosNeg_Counter = Counter(test_PosNeg)\n",
    "\n",
    "# '0'=negative, '1'=positive\n",
    "training_pos_value = training_PosNeg_Counter['1']/sum(training_PosNeg_Counter.values())\n",
    "print(\"training set positive percentage: {:.2%}\".format(training_pos_value))\n",
    "\n",
    "training_neg_value = training_PosNeg_Counter['0']/sum(training_PosNeg_Counter.values())\n",
    "print(\"training set negative percentage: {:.2%}\".format(training_neg_value))\n",
    "\n",
    "devtest_pos_value = devtest_PosNeg_Counter['1']/sum(devtest_PosNeg_Counter.values())\n",
    "print(\"devtest set positive percentage: {:.2%}\".format(devtest_pos_value))\n",
    "\n",
    "devtest_neg_value = devtest_PosNeg_Counter['0']/sum(devtest_PosNeg_Counter.values())\n",
    "print(\"devtest set negative percentage: {:.2%}\".format(devtest_neg_value))\n",
    "\n",
    "test_pos_value = test_PosNeg_Counter['1']/sum(test_PosNeg_Counter.values())\n",
    "print(\"test set positive percentage: {:.2%}\".format(test_pos_value))\n",
    "\n",
    "test_neg_value = test_PosNeg_Counter['0']/sum(test_PosNeg_Counter.values())\n",
    "print(\"test set negative percentage: {:.2%}\".format(test_neg_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (2 marks) - Some simple data exploration\n",
    "Answer the following questions. In your solution you need to include the code that you used to answer the questions. To find the answers to all of the questions below, **do not convert the text to lowercase**.\n",
    "\n",
    "1. (1 mark) What is the size of the entire vocabulary in the training set?\n",
    "2. (1 mark) In the training set, what words appear in the largest number of tweets with positive sentiment? What words appear in the largest number of tweets with negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  346623 \n",
      "\n",
      "Words which appear in largest number of tweets with positive sentiment: \n",
      "@  !  .  the  to  I  ,  a  you  and  \n",
      "\n",
      "Words which appear in largest number of tweets with negative sentiment: \n",
      "@  .  I  to  the  !  ,  a  my  i  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_words = []\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "for (i,j,k,l) in training_set_tokenized:\n",
    "    for word in set(l):\n",
    "        #add to total words\n",
    "        total_words.append(word)\n",
    "        if j =='0':\n",
    "            #its negative\n",
    "            negative_words.append(word)\n",
    "        if j =='1':\n",
    "            #its positive\n",
    "            positive_words.append(word)\n",
    "        \n",
    "unique_total_words = set(total_words)\n",
    "positive_words_counter = Counter(positive_words)\n",
    "negative_words_counter = Counter(negative_words)\n",
    "\n",
    "print(\"Size of vocabulary: \", len(unique_total_words), \"\\n\")\n",
    "\n",
    "print(\"Words which appear in largest number of tweets \" \\\n",
    "      \"with positive sentiment: \")\n",
    "for (i,j) in positive_words_counter.most_common(10):\n",
    "    print(i, \" \", end=\"\")\n",
    "print(\"\\n\") #newline\n",
    "\n",
    "print(\"Words which appear in largest number of tweets \" \\\n",
    "      \"with negative sentiment: \")\n",
    "for (i,j) in negative_words_counter.most_common(10):\n",
    "    print(i, \" \", end=\"\")\n",
    "print(\"\\n\") #newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (2 marks) - One-hot encoding for Naive Bayes in NLTK\n",
    "\n",
    "Using the training set, design a feature extractor that uses one-hot encoding with the entire vocabulary in the training set. Use the feature extractor to train a Naive Bayes classifier in NLTK and report the accuracy of the classifier using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:  0.86886\n",
      "devtest set:  0.7662\n",
      "test set:  0.76926\n"
     ]
    }
   ],
   "source": [
    "def feature_extractor(words):\n",
    "    result = dict()\n",
    "    for w in words:\n",
    "        result[w] = (w in unique_total_words)\n",
    "    return result\n",
    "\n",
    "train_features = [(feature_extractor(l),j) for (i,j,k,l) in training_set_tokenized]\n",
    "devtest_features = [(feature_extractor(l), j) for (i,j,k,l) in devtest_set_tokenized]\n",
    "test_features = [(feature_extractor(l), j) for (i,j,k,l) in test_set_tokenized]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "print(\"training set: \",nltk.classify.accuracy(classifier, train_features))\n",
    "print(\"devtest set: \",nltk.classify.accuracy(classifier, devtest_features))\n",
    "print(\"test set: \",nltk.classify.accuracy(classifier, test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 (2 marks) - One-hot encoding of most informative features\n",
    "\n",
    "Find the 2000 most informative features with the help of the NLTK classifier of exercise 4 (read [chapter 6 of the NLTK book](http://www.nltk.org/book/ch06.html) for help on how to find the most informative features). Use NLTK to build a new Naive Bayes classifier that uses these 2000 most informative features, train it on the training set, and report the accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:  0.578395\n",
      "devtest set:  0.57246\n",
      "test set:  0.56868\n"
     ]
    }
   ],
   "source": [
    "most_informative2000 = classifier.most_informative_features(2000)\n",
    "most_informative2000words = []\n",
    "for (word,boolean) in most_informative2000:\n",
    "    most_informative2000words.append(word)\n",
    "\n",
    "def FE_most_informative_features(words):\n",
    "    result = dict()\n",
    "    for w in words:\n",
    "        if w in most_informative2000words:\n",
    "            result[w] = (w in word)\n",
    "    return result\n",
    "\n",
    "train_features = [(FE_most_informative_features(l),j) for (i,j,k,l) in training_set_tokenized]\n",
    "devtest_features = [(FE_most_informative_features(l), j) for (i,j,k,l) in devtest_set_tokenized]\n",
    "test_features = [(FE_most_informative_features(l), j) for (i,j,k,l) in test_set_tokenized]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "print(\"training set: \",nltk.classify.accuracy(classifier, train_features))\n",
    "print(\"devtest set: \",nltk.classify.accuracy(classifier, devtest_features))\n",
    "print(\"test set: \",nltk.classify.accuracy(classifier, test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 (2 marks) - Tfidf for Naive Bayes in Scikit-Learn\n",
    "Using Scikit-Learn, generate the tf.idf matrix of the training set. **Use the defaults of `sklearn`'s `TfidfVectorizer` except for `lowercase`, which you must set to `False`**. [sklearn documentation for TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) With this matrix, train an `sklearn` Naive Bayes classifier using the training set and report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  0.85102\n",
      "devtest:  0.7663\n",
      "test:  0.76866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=\"False\")\n",
    "tfidf_train = tfidf.fit_transform([l for (i,j,k,l) in training_set])\n",
    "tfidf_devtest = tfidf.transform([l for (i,j,k,l) in devtest_set])\n",
    "tfidf_test = tfidf.transform(([l for (i,j,k,l) in test_set]))\n",
    "\n",
    "tfidfclassifier = MultinomialNB()\n",
    "tfidfclassifier.fit(tfidf_train,[j for (i,j,k,l) in training_set])\n",
    "\n",
    "print(\"test: \",accuracy_score([j for (i,j,k,l) in training_set], tfidfclassifier.predict(tfidf_train)))\n",
    "print(\"devtest: \",accuracy_score([j for (i,j,k,l) in devtest_set], tfidfclassifier.predict(tfidf_devtest)))\n",
    "print(\"test: \",accuracy_score([j for (i,j,k,l) in test_set], tfidfclassifier.predict(tfidf_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 (2 marks) - Analysis of Results\n",
    "Analyse the results of all the classifiers from the previous exercises, and answer these questions. In all answers you must include any code that you used to answer the questions, the output of the code. Your answer must also include text formatted in Markdown where you write your interpretation of the output and how this interpretation answer the questions.\n",
    "\n",
    "1. (1 mark) Did you observe any overfitting in any of the classifiers? How did you determine whether they are overfitting?\n",
    "2. (1 mark) Do we have too little training data, or do we have too much training data for these classifiers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.\n",
    "Yes, both the: **One-hot encoding for Naive Bayes in NLTK** and **Tfidf for Naive Bayes in Scikit-Learn** are overfitting.  This can be caused by providing too many features and relying on the training data that doesn't generalise well to new examples.  The results of the classifiers reveal that they are overfitting.\n",
    "\n",
    "**One-hot encoding for Naive Bayes in NLTK**<br>\n",
    "Difference in accuracy between training set and devtest set: 0.10266<br>\n",
    "Difference in accuracy between training set and test set: 0.0996<br>\n",
    "\n",
    "|Set          |Accuracy |\n",
    "|-------------|---------|\n",
    "|training set:|0.86886  |\n",
    "|devtest set: |0.7662   |\n",
    "|test set:    |0.76926  |\n",
    "\n",
    "**Tfidf for Naive Bayes in Scikit-Learn**<br>\n",
    "Difference in accuracy between training set and devtest set: 0.08472<br>\n",
    "Difference in accuracy between training set and test set: 0.08236<br>\n",
    "\n",
    "|Set          |Accuracy|\n",
    "|-------------|------- |\n",
    "|training set:|0.85102 |\n",
    "|devtest:     |0.7663  |\n",
    "|test:        |0.76866 |\n",
    "\n",
    "#### 2.\n",
    "Yes, there is too much training data in the classifers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (8 marks) - Improve the System and Final Analysis\n",
    "This exercise is open ended. Your goal is to perform a more detailed error analysis and identify ways to improve the classification of sentiments by adding or changing the features. Read, for example, Chapter 7 of the book by Maynard et al, or do your own research using the library or the Web. To obtain top marks in this part, you need to include the following in the answer:\n",
    "\n",
    "1. An error analysis of the previous systems.\n",
    "2. Text explaining what sort of modifications you would want to implement, and justify why these would be useful modifications.\n",
    "3. An implementation of the improved classifier.\n",
    "4. An evaluation of the results and comparison with the previous classifiers. You must explain what is the best system. In your explanation, include all factors that you used to decide which system is best.\n",
    "5. Text explaining what further changes would possibly improve the classifier and why.\n",
    "\n",
    "All this information should be inserted in this notebook below this question. The information should be structured in sections and it must be clear and precise. The explanations should be convincing. Below is a possible list of section headings. These sections are just a guideline. Feel free to change them, but make sure that they are informative and relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Error Analysis\n",
    "\n",
    "**One-hot encoding for Naive Bayes in NLTK** and **Tfidf for Naive Bayes in Scikit-Learn** are over fitted.\n",
    "\n",
    "### 2. Explanation of the Proposed New Classifier\n",
    "##### Giving less data to the training set and more to dev-test and test set\n",
    "This can reduce overfitting.\n",
    "\n",
    "##### Removing punctuation characters from the vocabulary\n",
    "As seen in the words which appear in the largest number of tweets, punctuation characters appear.  The tokenizer has recognised these characters as words, however they are not technically words.  This can be seen with words with the largest number of tweets.\n",
    "Removing punctuation characters would help the classifier.\n",
    "\n",
    "##### Removing stopwords from the vocabulary\n",
    "Stopwords are high frequency words.  These words appear in both positive and negative sentiments.  Because of their high frequency, they can make the classifier inaccurate.  Removing stopwords will lower the vocabulary and increase efficiency\n",
    "\n",
    "\n",
    "### 3. Code of the Proposed New Classifier\n",
    "\n",
    "#### [SEE CODE BLOCK BELOW]\n",
    "`#Removing stopwords from the vocabulary`\n",
    "\n",
    "### 4. Evaluation and Comparison\n",
    "\n",
    "##### Removing stopwords from the vocabulary\n",
    "The vocabulary has been reduced from 346,623 to 321,588 which is a 25,035 reduction in words.\n",
    "\n",
    "### 5. Final Conclusions and Possible Improvements\n",
    "\n",
    "Both **One-hot encoding for Naive Bayes in NLTK** and **Tfidf for Naive Bayes in Scikit-Learn** achieve similar goals but take different approaches.  The more a classifer can be trained, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nigel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Size of vocabulary:  321588 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing stopwords from the vocabulary\n",
    "import collections\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "total_words = []\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "for (i,j,k,l) in training_set_tokenized:\n",
    "    if word not in set(l):\n",
    "        for word in set(l):\n",
    "            #add to total words\n",
    "            total_words.append(word)\n",
    "            if j =='0':\n",
    "                #its negative\n",
    "                negative_words.append(word)\n",
    "            if j =='1':\n",
    "                #its positive\n",
    "                positive_words.append(word)\n",
    "\n",
    "unique_total_words = set(total_words)\n",
    "positive_words_counter = Counter(positive_words)\n",
    "negative_words_counter = Counter(negative_words)\n",
    "\n",
    "print(\"Size of vocabulary: \", len(unique_total_words), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your submission should consist of this jupyter notebook with all your code and explanations inserted in the notebook. The notebook should contain the output of the runs so that it can be read by the assessor without needing to run the output.\n",
    "\n",
    "Examine this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the markdown notation](http://daringfireball.net/projects/markdown/syntax), which explains the format of the text.\n",
    "\n",
    "Late submissions will have a penalty of **4 marks deduction per day late**.\n",
    "\n",
    "Each question specifies a mark. The final mark of the assignment is the sum of all the individual marks, after applying any deductions for late submission.\n",
    "\n",
    "By submitting this assignment you are acknowledging that this is your own work. Any submissions that breach the code of academic honesty will be penalised as per the [academic honesty policy](https://staff.mq.edu.au/work/strategy-planning-and-governance/university-policies-and-procedures/policies/academic-honesty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
